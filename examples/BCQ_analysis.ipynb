{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compressed = torch.load(\"../layer30_v_proj_weight_packed.pt\",map_location=torch.device('cpu'))\n",
    "#quantized = torch.load(\"../layer30_v_proj_weight.pt\",map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Packer:\n",
    "    def __init__(self):\n",
    "        self.s = torch.from_numpy(np.array([1, 2, 4, 8, 16, 32, 64, 128])).view(\n",
    "            [-1, 1])\n",
    "        if torch.cuda.is_available():\n",
    "            self.s = self.s.cuda()\n",
    "        self.w_pool = {}\n",
    "\n",
    "    def __get_weight(self, shape, dtype):\n",
    "        key = np.prod(shape)\n",
    "        if key not in self.w_pool.keys():\n",
    "            self.w_pool[key] = torch.zeros(shape, dtype=dtype)\n",
    "            if torch.cuda.is_available():\n",
    "                self.w_pool[key] = self.w_pool[key].cuda()\n",
    "        return self.w_pool[key].reshape(shape)\n",
    "\n",
    "    def pack(self, b):\n",
    "        shape = b.shape\n",
    "        p_b = b\n",
    "        print(\"!!!!\\n\",b)\n",
    "        if torch.cuda.is_available():\n",
    "            p_b = p_b.cuda()\n",
    "        p_b = (p_b + 1) / 2  # (-1., +1.) -> (0, 1)\n",
    "        p_b = torch.reshape(p_b, [8, -1]).type(torch.uint8)\n",
    "        print(\"shape 8 \",p_b.shape)\n",
    "        p_b = p_b * self.s\n",
    "        p_b = p_b.sum(0)\n",
    "        p_b = p_b.type(torch.uint8)\n",
    "        return p_b, shape\n",
    "\n",
    "    def unpack(self, pb, shape, dtype=torch.float16):\n",
    "        b = self.__get_weight(shape, dtype).view([8, -1])\n",
    "        for i in range(8):\n",
    "            b[i] = (pb & 1)  # (pB%2)\n",
    "            pb = pb >> 1  # //2\n",
    "        b = b * 2 - 1\n",
    "        b = b.reshape(shape)\n",
    "        return b\n",
    "\n",
    "\n",
    "PACKER = Packer()\n",
    "\n",
    "class CompressionParameter(nn.Parameter):\n",
    "    def compress(self, **kwargs):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def decompress(self, **kwargs):\n",
    "        raise NotImplemented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_quantize_tensor(\n",
    "    w, n_bit=4, zero_point=True, q_group_size=128, inplace=False, get_scale_zp=True\n",
    "):\n",
    "    org_w_shape = w.shape\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "    assert w.dim() == 2\n",
    "    if zero_point:\n",
    "        max_val = w.amax(dim=1, keepdim=True)\n",
    "        min_val = w.amin(dim=1, keepdim=True)\n",
    "        max_int = 2**n_bit - 1\n",
    "        min_int = 0\n",
    "        scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "        zeros = (-torch.round(min_val / scales)).clamp_(min_int, max_int)\n",
    "    else:  # we actually never used this\n",
    "        assert min_val is None\n",
    "        max_val = w.abs().amax(dim=1, keepdim=True)\n",
    "        max_val = max_val.clamp(min=1e-5)\n",
    "        max_int = 2 ** (n_bit - 1) - 1\n",
    "        min_int = -(2 ** (n_bit - 1))\n",
    "        scales = max_val / max_int\n",
    "        zeros = 0\n",
    "\n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    if inplace:\n",
    "        (\n",
    "            (w.div_(scales).round_().add_(zeros)).clamp_(min_int, max_int).sub_(zeros)\n",
    "        ).mul_(scales)\n",
    "    else:\n",
    "        w = (\n",
    "            torch.clamp(torch.round(w / scales) + zeros, min_int, max_int) - zeros\n",
    "        ) * scales\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    w = w.reshape(org_w_shape)\n",
    "    print(scales.shape)\n",
    "    scales = scales.view(w.shape[0], -1)\n",
    "    zeros = zeros.view(w.shape[0], -1)\n",
    "    intweight = []\n",
    "    scale_zeros = zeros * scales\n",
    "    for idx in range(w.shape[1]):\n",
    "        intweight.append(\n",
    "            torch.round(\n",
    "                (w[:, idx] + scale_zeros[:, idx // q_group_size])\n",
    "                / scales[:, idx // q_group_size]\n",
    "            ).to(torch.int)[:, None]\n",
    "        )\n",
    "    intweight = torch.cat(intweight, dim=1)\n",
    "    # intweight = intweight.t().contiguous()\n",
    "    intweight = intweight.to(dtype=torch.int32).reshape([intweight.shape[0], -1, q_group_size])\n",
    "    scales = scales.view(w.shape[0], -1,1)\n",
    "    zeros = zeros.view(w.shape[0], -1,1)\n",
    "    if get_scale_zp:\n",
    "        return w, scales, zeros, intweight\n",
    "    else:\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "#from clet.functions.rtn import Quantizer\n",
    "from utils import CompressionParameter, PACKER, Quantizer\n",
    "from bcq_parameter import BCQParameter\n",
    "\n",
    "\n",
    "class RTNParameter(CompressionParameter):\n",
    "    def compress(self, in_ch_wise=False, **kwargs):\n",
    "        data_shape = self.data.shape\n",
    "        group_size = -1\n",
    "        if 'group_size' in kwargs:\n",
    "            group_size = kwargs.pop('group_size')\n",
    "        out_ch = data_shape[0]\n",
    "        in_ch = data_shape[1]\n",
    "\n",
    "        quant = Quantizer()\n",
    "        quant.configure(**kwargs)\n",
    "        if in_ch_wise == False:\n",
    "            data = self.data\n",
    "            if group_size > 0:\n",
    "                data = data.reshape([-1, group_size])\n",
    "            quant.find_params(data, weight=True)\n",
    "            quant_data  = torch.clamp(torch.round(data / quant.scale) + quant.zero, 0, quant.maxq)\n",
    "            quant_data  = quant_data.reshape([out_ch, -1]).to(torch.int)\n",
    "            quant.scale = quant.scale.reshape([out_ch, -1, 1])\n",
    "            quant.zero  = quant.zero.reshape([out_ch, -1, 1])\n",
    "        else:\n",
    "            data = self.data.T\n",
    "            if group_size > 0:\n",
    "                data = data.reshape([-1, group_size])\n",
    "            quant.find_params(data, weight=True)\n",
    "            quant_data = torch.clamp(torch.round(data / quant.scale) + quant.zero, 0, quant.maxq)\n",
    "            quant_data = quant_data.reshape([in_ch, -1, group_size]).to(torch.int)\n",
    "            quant.scale = quant.scale.reshape([in_ch, -1, 1])\n",
    "            quant.zero  = quant.zero.reshape([in_ch, -1, 1])\n",
    "\n",
    "        return quant.scale, quant.zero, quant_data, quant_data.shape\n",
    "    \n",
    "    def decompress(self, scale, zero, quant_data, quant_data_shape, in_ch_wise=False):\n",
    "        # w.shape = [out_ch, in_ch]\n",
    "        # in_ch_wise == True\n",
    "        #   -> quant_data.shape = [in_ch, out_ch//group_size, group_size]\n",
    "        #   -> scale.shape      = [in_ch, out_ch//group_size, 1]\n",
    "        #   -> zero.shape       = [in_ch, out_ch//group_size, 1]\n",
    "        # in_ch_wise == False\n",
    "        #   -> quant_data.shape = [out_ch, in_ch//group_size, group_size]\n",
    "        #   -> scale.shape      = [out_ch, in_ch//group_size, 1]\n",
    "        #   -> zero.shape       = [out_ch, in_ch//group_size, 1]\n",
    "\n",
    "        if in_ch_wise == True:\n",
    "            out_ch = quant_data_shape[1] * quant_data_shape[2]\n",
    "            decomp_w = scale * (quant_data - zero)\n",
    "            decomp_w = decomp_w.reshape([-1, out_ch]).T\n",
    "        else:\n",
    "            out_ch = quant_data_shape[0]\n",
    "            decomp_w = scale * (quant_data - zero)\n",
    "            decomp_w = decomp_w.reshape([out_ch, -1])\n",
    "        self.data = decomp_w\n",
    "\n",
    "    def convert_bcq_format(self, scale, zero, quant_data, qbits, do_packing=False, in_ch_wise=False):\n",
    "        global PACKER\n",
    "\n",
    "        zero   = scale * zero\n",
    "        upack  = torch.Tensor([[2**i for i in range(qbits)]])\n",
    "        scale  = scale / 2.0\n",
    "        print(\"bf matmul\",scale.shape)\n",
    "        scale  = torch.matmul(scale, upack)\n",
    "        print(\"af matmul\",scale.shape,upack.shape)\n",
    "        offset = scale.sum(-1).unsqueeze(-1) - zero\n",
    "\n",
    "        binary = torch.zeros(list(quant_data.shape) + [qbits])\n",
    "        binary_shape = binary.shape\n",
    "        print(binary_shape)\n",
    "        for i in range(qbits):\n",
    "            binary[:, :, i] = ((quant_data >> i) & 1) * 2.0 - 1.0\n",
    "\n",
    "        scale = scale.permute(1,2,0)\n",
    "        binary = binary.permute(1,2,0)\n",
    "        offset = offset.permute(1,0,2)\n",
    "        K = binary.shape[0] #input\n",
    "        \n",
    "        binary = binary.reshape(K,qbits,-1)\n",
    "\n",
    "        N = binary.shape[2] #output\n",
    "\n",
    "        bW = torch.zeros([K // 32, qbits, N], dtype=torch.int32)\n",
    "        binary_shape = binary.shape\n",
    "        if do_packing == True:\n",
    "            for n in range(N):\n",
    "                for b in range(qbits):\n",
    "                    for k in range(0, K, 32):\n",
    "                        s = 0\n",
    "                        for t in range(32):\n",
    "                            if binary[k + t][b][n] == 1:\n",
    "                                s |= 1 << t  # 비트를 설정\n",
    "                        bW[k // 32][b][n] = (s & 0xFFFFFFFF)\n",
    "\n",
    "        return scale, bW, binary_shape, offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  1,  2,  3],\n",
      "          [ 4,  5,  6,  7],\n",
      "          [ 8,  9, 10, 11]],\n",
      "\n",
      "         [[12, 13, 14, 15],\n",
      "          [16, 17, 18, 19],\n",
      "          [20, 21, 22, 23]]],\n",
      "\n",
      "\n",
      "        [[[24, 25, 26, 27],\n",
      "          [28, 29, 30, 31],\n",
      "          [32, 33, 34, 35]],\n",
      "\n",
      "         [[36, 37, 38, 39],\n",
      "          [40, 41, 42, 43],\n",
      "          [44, 45, 46, 47]]],\n",
      "\n",
      "\n",
      "        [[[48, 49, 50, 51],\n",
      "          [52, 53, 54, 55],\n",
      "          [56, 57, 58, 59]],\n",
      "\n",
      "         [[60, 61, 62, 63],\n",
      "          [64, 65, 66, 67],\n",
      "          [68, 69, 70, 71]]]])\n",
      "tensor([[[[ 0,  4,  8],\n",
      "          [12, 16, 20]],\n",
      "\n",
      "         [[ 1,  5,  9],\n",
      "          [13, 17, 21]],\n",
      "\n",
      "         [[ 2,  6, 10],\n",
      "          [14, 18, 22]],\n",
      "\n",
      "         [[ 3,  7, 11],\n",
      "          [15, 19, 23]]],\n",
      "\n",
      "\n",
      "        [[[24, 28, 32],\n",
      "          [36, 40, 44]],\n",
      "\n",
      "         [[25, 29, 33],\n",
      "          [37, 41, 45]],\n",
      "\n",
      "         [[26, 30, 34],\n",
      "          [38, 42, 46]],\n",
      "\n",
      "         [[27, 31, 35],\n",
      "          [39, 43, 47]]],\n",
      "\n",
      "\n",
      "        [[[48, 52, 56],\n",
      "          [60, 64, 68]],\n",
      "\n",
      "         [[49, 53, 57],\n",
      "          [61, 65, 69]],\n",
      "\n",
      "         [[50, 54, 58],\n",
      "          [62, 66, 70]],\n",
      "\n",
      "         [[51, 55, 59],\n",
      "          [63, 67, 71]]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(3*2*3*4).reshape(3,2,3,4)  # shape: (2, 3, 4, 5)\n",
    "\n",
    "# 차원 순서 변경 (0, 3, 1, 2)\n",
    "permuted_tensor = tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "print(permuted_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([255], dtype=torch.uint8)\n",
      "quant torch.Size([1024, 32, 1]) torch.Size([1024, 32, 1]) torch.Size([1024, 4096])\n",
      "bf matmul torch.Size([1024, 32, 1])\n",
      "af matmul torch.Size([1024, 32, 4]) torch.Size([1, 4])\n",
      "torch.Size([1024, 4096, 4])\n",
      "torch.Size([128, 4, 1024]) torch.Size([32, 4, 1024]) torch.Size([32, 1024, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "#from clet.functions.rtn import Quantizer\n",
    "from utils import CompressionParameter, PACKER, Quantizer\n",
    "from bcq_parameter import BCQParameter\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    w_org = torch.randn(1024,4096)\n",
    "    #w =torch.tensor([511]).to(torch.int16)\n",
    "    #print(w.to(torch.uint8))\n",
    "    #w_org = torch.arange(4 * 32).reshape(4, 32).to(torch.float16)\n",
    "    #print(w_org)\n",
    "    # INT4 Quantization -> RTN\n",
    "    w_rtn = RTNParameter(w_org)\n",
    "    scale, zero, w_quant, w_quant_shape = w_rtn.compress(in_ch_wise=False, qbits=4, group_size=128, perchannel=True, sym=False)\n",
    "    #print(w_quant)\n",
    "    #w, s, z ,i= pseudo_quantize_tensor(w_org)\n",
    "    #a = scale*(w_quant-zero)\n",
    "    print(\"quant\",scale.shape,zero.shape,w_quant.shape)\n",
    "    #print(scale.dtype,zero.dtype,w_quant.dtype)\n",
    "    #print(\"!!!\",w_quant.shape[1])\n",
    "    #print(zero)\n",
    "    #print(scale)\n",
    "    #print(w_quant)\n",
    "    #print(a)\n",
    "    #print(z)\n",
    "\n",
    "    #w_rtn.decompress(scale, zero, w_quant, w_quant_shape, in_ch_wise=False)\n",
    "    #print(abs(w_org-w_rtn.data).mean())\n",
    "\n",
    "    # Convert INT4 -> BCQ4\n",
    "    alpha, binary, binary_shape, offset = w_rtn.convert_bcq_format(scale, zero, w_quant, qbits=4, do_packing=False, in_ch_wise=False)\n",
    "    \n",
    "    print(binary.size(),alpha.size(),offset.size())\n",
    "    #print(alpha)\n",
    "    #print(binary)\n",
    "    #print(offset)\n",
    "    # BCQ Decompress Check\n",
    "    #w_bcq = BCQParameter(w_org)\n",
    "    #w_bcq.decompress(alpha, binary, binary_shape, offset=offset, do_packing=True, in_ch_wise=False)\n",
    "    #print(abs(w_bcq.data - w_rtn.data).mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
