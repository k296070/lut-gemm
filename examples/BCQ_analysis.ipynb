{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = torch.load(\"../layer30_v_proj_weight_packed.pt\",map_location=torch.device('cpu'))\n",
    "quantized = torch.load(\"../layer30_v_proj_weight.pt\",map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_quantize_tensor(\n",
    "    w, n_bit=4, zero_point=True, q_group_size=128, inplace=False, get_scale_zp=True\n",
    "):\n",
    "    org_w_shape = w.shape\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "    assert w.dim() == 2\n",
    "    if zero_point:\n",
    "        max_val = w.amax(dim=1, keepdim=True)\n",
    "        min_val = w.amin(dim=1, keepdim=True)\n",
    "        max_int = 2**n_bit - 1\n",
    "        min_int = 0\n",
    "        scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "        zeros = (-torch.round(min_val / scales)).clamp_(min_int, max_int)\n",
    "    else:  # we actually never used this\n",
    "        assert min_val is None\n",
    "        max_val = w.abs().amax(dim=1, keepdim=True)\n",
    "        max_val = max_val.clamp(min=1e-5)\n",
    "        max_int = 2 ** (n_bit - 1) - 1\n",
    "        min_int = -(2 ** (n_bit - 1))\n",
    "        scales = max_val / max_int\n",
    "        zeros = 0\n",
    "\n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    if inplace:\n",
    "        (\n",
    "            (w.div_(scales).round_().add_(zeros)).clamp_(min_int, max_int).sub_(zeros)\n",
    "        ).mul_(scales)\n",
    "    else:\n",
    "        w = (\n",
    "            torch.clamp(torch.round(w / scales) + zeros, min_int, max_int) - zeros\n",
    "        ) * scales\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    w = w.reshape(org_w_shape)\n",
    "    print(scales.shape)\n",
    "    scales = scales.view(w.shape[0], -1)\n",
    "    zeros = zeros.view(w.shape[0], -1)\n",
    "    intweight = []\n",
    "    scale_zeros = zeros * scales\n",
    "    for idx in range(w.shape[1]):\n",
    "        intweight.append(\n",
    "            torch.round(\n",
    "                (w[:, idx] + scale_zeros[:, idx // q_group_size])\n",
    "                / scales[:, idx // q_group_size]\n",
    "            ).to(torch.int)[:, None]\n",
    "        )\n",
    "    intweight = torch.cat(intweight, dim=1)\n",
    "    # intweight = intweight.t().contiguous()\n",
    "    intweight = intweight.to(dtype=torch.int32).reshape([intweight.shape[0], -1, q_group_size])\n",
    "    scales = scales.view(w.shape[0], -1,1)\n",
    "    zeros = zeros.view(w.shape[0], -1,1)\n",
    "    if get_scale_zp:\n",
    "        return w, scales, zeros, intweight\n",
    "    else:\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(self, in_ch_wise=False, **kwargs):\n",
    "    data_shape = self.data.shape\n",
    "    group_size = -1\n",
    "    if 'group_size' in kwargs:\n",
    "        group_size = kwargs.pop('group_size')\n",
    "    out_ch = data_shape[0]\n",
    "    in_ch = data_shape[1]\n",
    "\n",
    "    quant = Quantizer()\n",
    "    quant.configure(**kwargs)\n",
    "    if in_ch_wise == False:\n",
    "        data = self.data\n",
    "        if group_size > 0:\n",
    "            data = data.reshape([-1, group_size])\n",
    "        quant.find_params(data, weight=True) #scale\n",
    "        quant_data  = torch.clamp(torch.round(data / quant.scale) + quant.zero, 0, quant.maxq)\n",
    "        quant_data  = quant_data.reshape([out_ch, -1, group_size]).to(torch.int)\n",
    "        quant.scale = quant.scale.reshape([out_ch, -1, 1])\n",
    "        quant.zero  = quant.zero.reshape([out_ch, -1, 1])\n",
    "    else:\n",
    "        data = self.data.T\n",
    "        if group_size > 0:\n",
    "            data = data.reshape([-1, group_size])\n",
    "        quant.find_params(data, weight=True)\n",
    "        quant_data = torch.clamp(torch.round(data / quant.scale) + quant.zero, 0, quant.maxq)\n",
    "        quant_data = quant_data.reshape([in_ch, -1, group_size]).to(torch.int)\n",
    "        quant.scale = quant.scale.reshape([in_ch, -1, 1])\n",
    "        quant.zero  = quant.zero.reshape([in_ch, -1, 1])\n",
    "\n",
    "    return quant.scale, quant.zero, quant_data, quant_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "#from clet.functions.rtn import Quantizer\n",
    "from utils import CompressionParameter, PACKER, Quantizer\n",
    "from bcq_parameter import BCQParameter\n",
    "\n",
    "def pack(self, b):\n",
    "    shape = b.shape\n",
    "    p_b = b\n",
    "    if torch.cuda.is_available():\n",
    "        p_b = p_b.cuda()\n",
    "    p_b = (p_b + 1) / 2  # (-1., +1.) -> (0, 1)\n",
    "    p_b = torch.reshape(p_b, [8, -1]).type(torch.uint8)\n",
    "    p_b = p_b * self.s\n",
    "    p_b = p_b.sum(0)\n",
    "    p_b = p_b.type(torch.uint8)\n",
    "    return p_b, shape\n",
    "\n",
    "def convert_bcq_format(self, scale, zero, quant_data, qbits, do_packing=False, in_ch_wise=False):\n",
    "    global PACKER\n",
    "\n",
    "    zero   = scale * zero\n",
    "    upack  = torch.Tensor([[2**i for i in range(qbits)]])\n",
    "    scale  = scale / 2.0\n",
    "    scale  = torch.matmul(scale, upack)\n",
    "\n",
    "    offset = scale.sum(-1).unsqueeze(-1) - zero\n",
    "\n",
    "    binary = torch.zeros(list(quant_data.shape) + [qbits])\n",
    "    binary_shape = binary.shape\n",
    "    for i in range(qbits):\n",
    "        binary[:, :, :, i] = ((quant_data >> i) & 1) * 2.0 - 1.0\n",
    "\n",
    "    if do_packing == True:\n",
    "        binary, binary_shape = PACKER.pack(binary)\n",
    "        binary = binary.to(self.data.device)\n",
    "\n",
    "    return scale, binary, binary_shape, offset\n",
    "\n",
    "class RTNParameter(CompressionParameter):\n",
    "    def compress(self, in_ch_wise=False, **kwargs):\n",
    "        data_shape = self.data.shape\n",
    "        group_size = -1\n",
    "        if 'group_size' in kwargs:\n",
    "            group_size = kwargs.pop('group_size')\n",
    "        out_ch = data_shape[0]\n",
    "        in_ch = data_shape[1]\n",
    "\n",
    "        quant = Quantizer()\n",
    "        quant.configure(**kwargs)\n",
    "        if in_ch_wise == False:\n",
    "            data = self.data\n",
    "            if group_size > 0:\n",
    "                data = data.reshape([-1, group_size])\n",
    "            quant.find_params(data, weight=True) #scale\n",
    "            quant_data  = torch.clamp(torch.round(data / quant.scale) + quant.zero, 0, quant.maxq)\n",
    "            quant_data  = quant_data.reshape([out_ch, -1, group_size]).to(torch.int)\n",
    "            quant.scale = quant.scale.reshape([out_ch, -1, 1])\n",
    "            quant.zero  = quant.zero.reshape([out_ch, -1, 1])\n",
    "        else:\n",
    "            data = self.data.T\n",
    "            if group_size > 0:\n",
    "                data = data.reshape([-1, group_size])\n",
    "            quant.find_params(data, weight=True)\n",
    "            quant_data = torch.clamp(torch.round(data / quant.scale) + quant.zero, 0, quant.maxq)\n",
    "            quant_data = quant_data.reshape([in_ch, -1, group_size]).to(torch.int)\n",
    "            quant.scale = quant.scale.reshape([in_ch, -1, 1])\n",
    "            quant.zero  = quant.zero.reshape([in_ch, -1, 1])\n",
    "\n",
    "        return quant.scale, quant.zero, quant_data, quant_data.shape\n",
    "\n",
    "    def decompress(self, scale, zero, quant_data, quant_data_shape, in_ch_wise=False):\n",
    "        # w.shape = [out_ch, in_ch]\n",
    "        # in_ch_wise == True\n",
    "        #   -> quant_data.shape = [in_ch, out_ch//group_size, group_size]\n",
    "        #   -> scale.shape      = [in_ch, out_ch//group_size, 1]\n",
    "        #   -> zero.shape       = [in_ch, out_ch//group_size, 1]\n",
    "        # in_ch_wise == False\n",
    "        #   -> quant_data.shape = [out_ch, in_ch//group_size, group_size]\n",
    "        #   -> scale.shape      = [out_ch, in_ch//group_size, 1]\n",
    "        #   -> zero.shape       = [out_ch, in_ch//group_size, 1]\n",
    "\n",
    "        if in_ch_wise == True:\n",
    "            out_ch = quant_data_shape[1] * quant_data_shape[2]\n",
    "            decomp_w = scale * (quant_data - zero)\n",
    "            decomp_w = decomp_w.reshape([-1, out_ch]).T\n",
    "        else:\n",
    "            out_ch = quant_data_shape[0]\n",
    "            decomp_w = scale * (quant_data - zero)\n",
    "            decomp_w = decomp_w.reshape([out_ch, -1])\n",
    "        self.data = decomp_w\n",
    "\n",
    "    def convert_bcq_format(self, scale, zero, quant_data, qbits, do_packing=False, in_ch_wise=False):\n",
    "        global PACKER\n",
    "\n",
    "        zero   = scale * zero\n",
    "        upack  = torch.Tensor([[2**i for i in range(qbits)]])\n",
    "        scale  = scale / 2.0\n",
    "        scale  = torch.matmul(scale, upack)\n",
    "\n",
    "        offset = scale.sum(-1).unsqueeze(-1) - zero\n",
    "\n",
    "        binary = torch.zeros(list(quant_data.shape) + [qbits])\n",
    "        binary_shape = binary.shape\n",
    "        for i in range(qbits):\n",
    "            binary[:, :, :, i] = ((quant_data >> i) & 1) * 2.0 - 1.0\n",
    "\n",
    "        if do_packing == True:\n",
    "            binary, binary_shape = PACKER.pack(binary)\n",
    "            binary = binary.to(self.data.device)\n",
    "\n",
    "        return scale, binary, binary_shape, offset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([716800, 1])\n",
      "torch.Size([5120, 140, 1]) torch.Size([5120, 140, 1]) torch.Size([5120, 140, 128])\n",
      "torch.float32 torch.float32 torch.int32\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]])\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'torch.Size' and 'torch.Size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(zero\u001b[38;5;241m-\u001b[39mz)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(scale\u001b[38;5;241m-\u001b[39ms)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mw_quant\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#print(z)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#w_rtn.decompress(scale, zero, w_quant, w_quant_shape, in_ch_wise=False)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#w_bcq.decompress(alpha, binary, binary_shape, offset=offset, do_packing=True, in_ch_wise=False)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#print(abs(w_bcq.data - w_rtn.data).mean())\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'torch.Size' and 'torch.Size'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    w_org = torch.randn(5120,17920)\n",
    "\n",
    "    # INT4 Quantization -> RTN\n",
    "    w_rtn = RTNParameter(w_org)\n",
    "    scale, zero, w_quant, w_quant_shape = w_rtn.compress(in_ch_wise=False, qbits=4, group_size=128, perchannel=True, sym=False)\n",
    "    w, s, z ,i= pseudo_quantize_tensor(w_org)\n",
    "\n",
    "    print(scale.shape,zero.shape,w_quant.shape)\n",
    "    print(scale.dtype,zero.dtype,w_quant.dtype)\n",
    "\n",
    "    print(zero-z)\n",
    "    print(scale-s)\n",
    "    print(i-w_quant)\n",
    "\n",
    "    #print(z)\n",
    "\n",
    "    #w_rtn.decompress(scale, zero, w_quant, w_quant_shape, in_ch_wise=False)\n",
    "    #print(abs(w_org-w_rtn.data).mean())\n",
    "\n",
    "    # Convert INT4 -> BCQ4\n",
    "    #alpha, binary, binary_shape, offset = w_rtn.convert_bcq_format(scale, zero, w_quant, qbits=3, do_packing=True, in_ch_wise=False)\n",
    "    \n",
    "    #print(binary.size(),alpha.size(),offset.size())\n",
    "    \n",
    "    #print(zero)\n",
    "\n",
    "    #print(offset)\n",
    "    # BCQ Decompress Check\n",
    "    #w_bcq = BCQParameter(w_org)\n",
    "    #w_bcq.decompress(alpha, binary, binary_shape, offset=offset, do_packing=True, in_ch_wise=False)\n",
    "    #print(abs(w_bcq.data - w_rtn.data).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
