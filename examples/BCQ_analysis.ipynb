{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = torch.load(\"../layer30_v_proj_weight_packed.pt\",map_location=torch.device('cpu'))\n",
    "quantized = torch.load(\"../layer30_v_proj_weight.pt\",map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5120, 140, 1]) torch.Size([5120, 140, 1]) torch.Size([5120, 140, 128])\n",
      "torch.Size([34406400]) torch.Size([5120, 140, 3]) torch.Size([5120, 140, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "#from clet.functions.rtn import Quantizer\n",
    "from utils import CompressionParameter, PACKER, Quantizer\n",
    "from bcq_parameter import BCQParameter\n",
    "\n",
    "def pack(self, b):\n",
    "    shape = b.shape\n",
    "    p_b = b\n",
    "    if torch.cuda.is_available():\n",
    "        p_b = p_b.cuda()\n",
    "    p_b = (p_b + 1) / 2  # (-1., +1.) -> (0, 1)\n",
    "    p_b = torch.reshape(p_b, [8, -1]).type(torch.uint8)\n",
    "    p_b = p_b * self.s\n",
    "    p_b = p_b.sum(0)\n",
    "    p_b = p_b.type(torch.uint8)\n",
    "    return p_b, shape\n",
    "\n",
    "def convert_bcq_format(self, scale, zero, quant_data, qbits, do_packing=False, in_ch_wise=False):\n",
    "    global PACKER\n",
    "\n",
    "    zero   = scale * zero\n",
    "    upack  = torch.Tensor([[2**i for i in range(qbits)]])\n",
    "    scale  = scale / 2.0\n",
    "    scale  = torch.matmul(scale, upack)\n",
    "\n",
    "    offset = scale.sum(-1).unsqueeze(-1) - zero\n",
    "\n",
    "    binary = torch.zeros(list(quant_data.shape) + [qbits])\n",
    "    binary_shape = binary.shape\n",
    "    for i in range(qbits):\n",
    "        binary[:, :, :, i] = ((quant_data >> i) & 1) * 2.0 - 1.0\n",
    "\n",
    "    if do_packing == True:\n",
    "        binary, binary_shape = PACKER.pack(binary)\n",
    "        binary = binary.to(self.data.device)\n",
    "\n",
    "    return scale, binary, binary_shape, offset\n",
    "\n",
    "class RTNParameter(CompressionParameter):\n",
    "    def compress(self, in_ch_wise=False, **kwargs):\n",
    "        data_shape = self.data.shape\n",
    "        group_size = -1\n",
    "        if 'group_size' in kwargs:\n",
    "            group_size = kwargs.pop('group_size')\n",
    "        out_ch = data_shape[0]\n",
    "        in_ch = data_shape[1]\n",
    "\n",
    "        quant = Quantizer()\n",
    "        quant.configure(**kwargs)\n",
    "        if in_ch_wise == False:\n",
    "            data = self.data\n",
    "            if group_size > 0:\n",
    "                data = data.reshape([-1, group_size])\n",
    "            quant.find_params(data, weight=True) #scale\n",
    "            quant_data  = torch.clamp(torch.round(data / quant.scale) + quant.zero, 0, quant.maxq)\n",
    "            quant_data  = quant_data.reshape([out_ch, -1, group_size]).to(torch.int)\n",
    "            quant.scale = quant.scale.reshape([out_ch, -1, 1])\n",
    "            quant.zero  = quant.zero.reshape([out_ch, -1, 1])\n",
    "        else:\n",
    "            data = self.data.T\n",
    "            if group_size > 0:\n",
    "                data = data.reshape([-1, group_size])\n",
    "            quant.find_params(data, weight=True)\n",
    "            quant_data = torch.clamp(torch.round(data / quant.scale) + quant.zero, 0, quant.maxq)\n",
    "            quant_data = quant_data.reshape([in_ch, -1, group_size]).to(torch.int)\n",
    "            quant.scale = quant.scale.reshape([in_ch, -1, 1])\n",
    "            quant.zero  = quant.zero.reshape([in_ch, -1, 1])\n",
    "\n",
    "        return quant.scale, quant.zero, quant_data, quant_data.shape\n",
    "\n",
    "    def decompress(self, scale, zero, quant_data, quant_data_shape, in_ch_wise=False):\n",
    "        # w.shape = [out_ch, in_ch]\n",
    "        # in_ch_wise == True\n",
    "        #   -> quant_data.shape = [in_ch, out_ch//group_size, group_size]\n",
    "        #   -> scale.shape      = [in_ch, out_ch//group_size, 1]\n",
    "        #   -> zero.shape       = [in_ch, out_ch//group_size, 1]\n",
    "        # in_ch_wise == False\n",
    "        #   -> quant_data.shape = [out_ch, in_ch//group_size, group_size]\n",
    "        #   -> scale.shape      = [out_ch, in_ch//group_size, 1]\n",
    "        #   -> zero.shape       = [out_ch, in_ch//group_size, 1]\n",
    "\n",
    "        if in_ch_wise == True:\n",
    "            out_ch = quant_data_shape[1] * quant_data_shape[2]\n",
    "            decomp_w = scale * (quant_data - zero)\n",
    "            decomp_w = decomp_w.reshape([-1, out_ch]).T\n",
    "        else:\n",
    "            out_ch = quant_data_shape[0]\n",
    "            decomp_w = scale * (quant_data - zero)\n",
    "            decomp_w = decomp_w.reshape([out_ch, -1])\n",
    "        self.data = decomp_w\n",
    "\n",
    "    def convert_bcq_format(self, scale, zero, quant_data, qbits, do_packing=False, in_ch_wise=False):\n",
    "        global PACKER\n",
    "\n",
    "        zero   = scale * zero\n",
    "        upack  = torch.Tensor([[2**i for i in range(qbits)]])\n",
    "        scale  = scale / 2.0\n",
    "        scale  = torch.matmul(scale, upack)\n",
    "\n",
    "        offset = scale.sum(-1).unsqueeze(-1) - zero\n",
    "\n",
    "        binary = torch.zeros(list(quant_data.shape) + [qbits])\n",
    "        binary_shape = binary.shape\n",
    "        for i in range(qbits):\n",
    "            binary[:, :, :, i] = ((quant_data >> i) & 1) * 2.0 - 1.0\n",
    "\n",
    "        if do_packing == True:\n",
    "            binary, binary_shape = PACKER.pack(binary)\n",
    "            binary = binary.to(self.data.device)\n",
    "\n",
    "        return scale, binary, binary_shape, offset\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    w_org = torch.randn(5120,17920)\n",
    "\n",
    "    # INT4 Quantization -> RTN\n",
    "    w_rtn = RTNParameter(w_org)\n",
    "    scale, zero, w_quant, w_quant_shape = w_rtn.compress(in_ch_wise=False, qbits=3, group_size=128, perchannel=True, sym=False)\n",
    "\n",
    "    print(scale.shape,zero.shape,w_quant.shape)\n",
    "\n",
    "    #w_rtn.decompress(scale, zero, w_quant, w_quant_shape, in_ch_wise=False)\n",
    "    #print(abs(w_org-w_rtn.data).mean())\n",
    "\n",
    "    # Convert INT4 -> BCQ4\n",
    "    alpha, binary, binary_shape, offset = w_rtn.convert_bcq_format(scale, zero, w_quant, qbits=3, do_packing=True, in_ch_wise=False)\n",
    "    \n",
    "    print(binary.size(),alpha.size(),offset.size())\n",
    "    \n",
    "    print(zero)\n",
    "\n",
    "    print(offset)\n",
    "    # BCQ Decompress Check\n",
    "    #w_bcq = BCQParameter(w_org)\n",
    "    #w_bcq.decompress(alpha, binary, binary_shape, offset=offset, do_packing=True, in_ch_wise=False)\n",
    "    #print(abs(w_bcq.data - w_rtn.data).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(self, in_ch_wise=False, **kwargs):\n",
    "    data_shape = self.data.shape\n",
    "    group_size = -1\n",
    "    if 'group_size' in kwargs:\n",
    "        group_size = kwargs.pop('group_size')\n",
    "    out_ch = data_shape[0]\n",
    "    in_ch = data_shape[1]\n",
    "\n",
    "    quant = Quantizer()\n",
    "    quant.configure(**kwargs)\n",
    "    if in_ch_wise == False:\n",
    "        data = self.data\n",
    "        if group_size > 0:\n",
    "            data = data.reshape([-1, group_size])\n",
    "        quant.find_params(data, weight=True) #scale\n",
    "        quant_data  = torch.clamp(torch.round(data / quant.scale) + quant.zero, 0, quant.maxq)\n",
    "        quant_data  = quant_data.reshape([out_ch, -1, group_size]).to(torch.int)\n",
    "        quant.scale = quant.scale.reshape([out_ch, -1, 1])\n",
    "        quant.zero  = quant.zero.reshape([out_ch, -1, 1])\n",
    "    else:\n",
    "        data = self.data.T\n",
    "        if group_size > 0:\n",
    "            data = data.reshape([-1, group_size])\n",
    "        quant.find_params(data, weight=True)\n",
    "        quant_data = torch.clamp(torch.round(data / quant.scale) + quant.zero, 0, quant.maxq)\n",
    "        quant_data = quant_data.reshape([in_ch, -1, group_size]).to(torch.int)\n",
    "        quant.scale = quant.scale.reshape([in_ch, -1, 1])\n",
    "        quant.zero  = quant.zero.reshape([in_ch, -1, 1])\n",
    "\n",
    "    return quant.scale, quant.zero, quant_data, quant_data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
